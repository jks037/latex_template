\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[numbers]{natbib}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage[colorlinks,linkcolor=blue]{hyperref}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex. (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
%%%%%%%%% TITLE
\title{A weak-shot classification practice}


\author{$\textnormal{Honghao Chen}^{*}$, $\textnormal{Tianyi Zhou}^{*}$\\
$^*$ Shanghai Jiao Tong University \\
}

\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

\end{abstract}

%%%%%%%%% BODY TEXT

\section{Introduction}
%（~\cite{chen2020weak}）
Deep learning has been successefully applied to a variaty of computer vision researchs and huge amout of powerful tools. However, the training of deep learning models relies much on well labeled datases. Though large commertial entities such as Google and Alibaba investigate a lot on collection of data sets, considering the endless fine-grained categories, the lack of well-labeled data sets is inevitable. A strategy alleviates this data-hungry problem is called weakly-supervised learning is to first gain informations from  fully-annotated training samples (base categories), then transfer the gained information to novel categories which have little or even no overlap with the original training samples.Previous research on this topic includes zero-shot learning, few-shot learning, weakly0supervised learning and semi-supervised learning. 

%zero-shot and few-shot learning intro, remain to modified according to related word section
Zero-shot learning and few-shot learning attempt to utility training samples with limited quantities. To solve the problem of distiction between base categories and nove categories, zero-shot learning extract category-leve semantic representations such as word vectors for all categories. Few-shot learning gains clue from small-sized clean examples (for example 5 or 10) in novel categories. Though succeed in some categories, they have several disadvantages:1. Annotating attributes needed for zero-shot learning and well-labeled samples (even few) are not always available;2.Attributes like word vectors are free, but are sometimes ambiguous due to the nature of language (for example one word has multiple meanings). These will affect the final effectiveness of training.

%some other related works. the pharagragh below is completely copied, remain to be replaced
Considering the drawbacks of zero/few-shot learning and the accessibility of free web data, we intend to learn novel
categories by virtue of web data with the support of a clean set of base categories, which is referred to as weak-shot
learning as illustrated in Figure 1. Formally, given a set of novel fine-grained categories which are not associated with
any clean training images, we collect web images for novel categories as weak-labeled images and meanwhile leverage the clean images from base fine-grained categories. We refer to the clean (resp., web) image set from base (resp., novel)
categories as base (resp., novel) training set. The closest related work to ours is (Niu, Veeraraghavan, and Sabharwal 2018), but they further assumed the reliability of word vectors (Mikolov et al. 2013; Pennington, Socher, and Manning arXiv:2009.09197v1 [cs.CV] 19 Sep 2020 2014) as well as the availability of unlabeled test images in the training stage.
In this learning scenario, the key issue of novel training set is label noise, which will significantly degrade the performance of learnt classifier (Niu, Veeraraghavan, and Sabharwal 2018; Zhuang et al. 2017; Patrini et al. 2017; Zhang,
Wang, and Qiao 2019). We explore using base training set to denoise novel training set, although they have disjoint category sets. As illustrated in Figure 2, our proposed framework employs the pairwise semantic similarity to bridge the
gap between base categories and novel categories. The pairwise similarity which denotes whether two images belong
to the same category is category-agnostic, so it is highly transferable across category sets even if they are disjoint.
Meanwhile, the pairwise similarity can be easily learnt fromlimited data, indicating that a small set of already annotated
images could help learn extensive novel categories.
Analogously, some methods (Chen et al. 2020; Oreshkin, Lopez, and Lacoste 2018) for few-shot learning transferred similarity from base categories to novel categories, which is directly used for classification. In contrast, we transfer the pairwise similarity to alleviate the label noise issue of web data. For learning from web data, some works (Sheng Guo and Huang 2018; Han, Luo, and Wang 2019) also attempted to denoise by similarity. But their similarities are derived from noisy samples (e.g., feature distances of a model pre-trained on web data), and likely to be corrupted due to noise overfitting, leading to sub-optimal results (Kuang-Huei Lee
and Yang 2018; Xiao et al. 2015).

% introduction to method and concludes
our proposals and results.


\section{Related Works}
\label{section:related}
Weak-shot classification is a new conception with few article available. In this section we introduce related work from two aspects.Section 2.1 to 2.4 introduce similar methods attempt to lower the requirement of training samples. Section 2.5 to 2.6 introduce other applications of weak-shot learning bisides weak-shot classification.

\subsection{Zero-Shot learning}
\label{section:related:zero}
Zero-shot learning employs category-level semantic representation (e.g., word vector or annotated attributes) to bridge the gap between seen (resp., base) categories and unseen (resp., novel) categories. A large part of works [6, 17, 38, 59] learn a mapping between visual features and category-level semantic representations. Recently, zero-shot object detection [73, 10], zero-shot semantic segmentation [5, 19, 60], and zero-shot instance segmentation [67] have also been explored. Zero-shot learning relies on category-level semantic representations for both base categories and novel categories, which are not required by weak-shot learning.
\subsection{few-Shot learning}
\subsection{Weakly-supervised learning}
\subsection{Webly-supervised learning}
\subsection{Weak-Shot Object Detection}
\subsection{Weak-Shot Instance Segmentation}


\section{Our Method}

\section{Experiments}

\section{Conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}
